---
title: "Weight Lifting Exercises"
author: "Cristofer Weber"
date: "February 21, 2015"
output:
  html_document:
    theme: united
bibliography: bibliography.bib
---

```{r, comment="Get data"}
trainingURL = url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")
testingURL = url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv")

if(!file.exists("data")) {
  dir.create("data")
}
setwd("data")
if(!file.exists("pml-training.csv")) {
  download.file(trainingURL, method = "curl", destfile = "pml-training.csv")
}

if(!file.exists("pml-testing.csv")) {
  download.file(testingURL, method = "curl", destfile = "pml-testing.csv")
}
```

```{r, comment="Load required packages, auxiliary functions, constants, and do the setup.", echo=FALSE, results='hide', warning=FALSE, message=FALSE}
require(stringr)
require(caret)
require(randomForest)
require(doMC)
require(ggplot2)
require(parallel)
require(plyr);
require(reshape2);
require(gridExtra);
require(gtable);
require(knitr);
require(xtable);

# Constants
clrs <- c("#E69F00", "#56B4E9", "#8B4513", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7", "#551A8B", "#999999")
columnsToDrop = c("kurtosis", "skewness", "max", "min", "amplitude", "var", "avg", "stddev")
otherUnwantedCols = c("X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "new_window", "num_window")

# Help Functions
simpleCap <- function(x) {
  s <- strsplit(x, " ")[[1]]
  paste(toupper(substring(s, 1,1)), substring(s, 2),
        sep="", collapse=" ")
}

classErrorPlot <- function(class.error,confusion.melt,celltextsize,fontsize){
  fontsize <- as.numeric(fontsize)
  g1 <- ggplot(data=class.error,aes(x=Activity,y=Error,group=Activity,colour=Activity,fill=Activity)) +
    theme_grey(base_size=fontsize) + theme(legend.position="top", legend.key.width=unit(0.1/nlevels(class.error$Activity),"npc")) +
    scale_fill_manual(values=c(clrs[1:(nlevels(class.error$Activity))])) + scale_colour_manual(values=c(clrs[1:(nlevels(class.error$Activity))])) +
    geom_bar(stat="identity")
  g2 <- ggplot(data=confusion.melt, aes(x = X, y = Y, fill = Count, label=Count)) + theme_grey(base_size=fontsize) + theme(legend.position="top", legend.key.width=unit(0.1,"npc")) +
    labs(x = "Predicted Class and Class Error", y = "Observed Class") +
    geom_raster() +
    scale_fill_gradient( low = "white", high = "purple", na.value="black", name = "Count" ) +
    geom_text(size=celltextsize) +
    geom_rect(size=1, fill=NA, colour="black",
              aes(xmin=length(levels(X))-0.5, xmax=length(levels(X))+0.5, ymin=1-0.5, ymax=length(levels(Y))+0.5)) +
    geom_rect(size=2, fill=NA, colour="black",
              aes(xmin=1-0.5, xmax=length(levels(X))+0.5, ymin=1-0.5, ymax=length(levels(Y))+0.5)) +
    scale_x_discrete(expand = c(0, 0)) +
    scale_y_discrete(expand = c(0, 0))
  gA <- ggplot_gtable(ggplot_build(g1))
  gB <- ggplot_gtable(ggplot_build(g2))
  gA$widths <- gB$widths
  grid.arrange(gA, gB, ncol=1)
}

importancePlot = function(d,ylb,fontsize){
  fontsize = as.numeric(fontsize)
  d = d[order(d[,2]),]
  d$Predictor = factor(as.character(d$Predictor),levels=rev(as.character(d$Predictor)))
  rownames(d) = NULL
  abs.min = abs(min(d[,2]))
  g1 = ggplot(data=d,aes_string(x="Predictor",y=ylb,group="Predictor",colour="Predictor",fill="Predictor")) + geom_bar(stat="identity") + theme_grey(base_size=fontsize)
  if(ylb=="mda") g1 = g1 + labs(y="Mean decrease in accuracy") else if(ylb=="mdg") g1 = g1 + labs(y="Mean decrease in Gini")
  g1 = g1 + theme(axis.text.x = element_text(angle=90,hjust=1,vjust=0.4)) + geom_hline(yintercept=abs.min,linetype="dashed",colour="black")
  print(g1)
}

# Setup
options(rf.cores=detectCores()-1, mc.cores=detectCores()-1)
registerDoMC(cores=3)
```


```{r, comment="Load activities dataset and keep only the set of selected predictors plus the resulting class", echo=FALSE, cache=TRUE}
headActivities = read.csv("data/pml-training.csv", header = T, comment.char = "", quote = "\"", nrows=5)
summaryActs = summary(headActivities)
dfSA = data.frame(summaryActs, stringsAsFactors = F)
activitiesNames = str_trim(unique(dfSA$Var2))

activities = read.csv("data/pml-training.csv", header=T, comment.char="", quote="\"", col.names = activitiesNames)
originalActivities = activities

# Drop all unwanted/unimportant columns
splittedNames = str_split(activitiesNames, pattern = "_")
prefixes = unlist(lapply(splittedNames, head, 1))
unwantedColumns = which(prefixes %in% columnsToDrop)
activities = activities[, -unwantedColumns]
activities = activities[, -which(names(activities) %in% otherUnwantedCols)]
```

## Summary
Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E). [@Velloso:2013:QAR:2459236.2459256]

As shown in the picture below, three sets of sensors were placed in different parts of the body using a Belt, a Glove and an Armband. Another set of sensors was placed in the Dumbbell. Each set of sensors has an Accelerometer, a Gyroscope and a Mgnetomete, and the collected metrics are the tridimentional raw data (X, Y and Z) from each sensor. Each measurement also has the calculated Euler angles:  Roll, Pitch and Yaw. All these metrics were recorded together with the activities (Classe), participant data, timestamps and other statistics.

<img alt="On-body sensing schema" src="http://groupware.les.inf.puc-rio.br/static/WLE/on-body-sensing-schema.png" height="20%" width="20%" align="middle">

More details of the experiment available at: http://groupware.les.inf.puc-rio.br/har#weight_lifting_exercises#ixzz3SObtcRBO

Participants of the class were asked to train a classifier able to identify the performed activity based on the recorded metrics, provided a training file and a test file. As part of the assignment, the test file is for evaluation only, so it can't be used during the training process. So I splitted the training file in two different sets, where the new train set is 75% of the original training file and the test set is 25% of the original training file, as below:

```{r, results='hide', message=FALSE}
set.seed(1234)
train = createDataPartition(activities$classe, p=0.75, list=F)
trainingActivities = activities[train,]
testingActivities = activities[-train,]
```

## Feature Selection and Preprocessing
### Empty values
```{r, results='hide', echo=FALSE}
statistics = sapply(columnsToDrop, simpleCap)
names(statistics) = NULL
```

Feature selection was performed in the new train set. As mentioned before, there are `r length(statistics)` statistics calculated per exercise repetition. Each of these statistical measures is blank most of the time, so I chose to drop every measure that matches one of the following names:
`r statistics`

A total of `r length(unwantedColumns)` measures were removed in this step.

### Unrelated Columns

From the remaining fields I also removed those that identify the participants, the repetitions, and the time windows:

```{r, echo=FALSE, results='asis'}
showOtherUnwantedCols = c(otherUnwantedCols, "")
matrixUnwantedCols = matrix(showOtherUnwantedCols, ncol=2, nrow=4, byrow = T)
rownames(matrixUnwantedCols) = NULL
colnames(matrixUnwantedCols) = NULL
knitr::kable(xtable(matrixUnwantedCols), col.names = c("Unrelated", "Unrelated"))
```

## Training

My learning method of choice was Random Forests. The model was trained using as predictors all raw and calculates measures collected during the activities, as below. 

```{r, comment="Train a Random Forest model using all selected predictors", echo=TRUE, cache=TRUE}
mForest = randomForest(y=trainingActivities$classe, x=trainingActivities[, -53], ntree=500, replace=T)
```

Here is the Training Classification Error per class, together with the Training Confusion Matrix.

```{r, comment="Plot training classification error and confusion matrix", echo=FALSE, message=FALSE, fig.align='center', fig.height=7}
confusion.dat <- data.frame(mForest$confusion)
names(confusion.dat)[ncol(confusion.dat)] <- "Error"

class.error <- confusion.dat[ncol(confusion.dat)]
class.error <- data.frame(Activity=rownames(class.error),Error=class.error[,1])
rownames(class.error) <- NULL

confusion.melt <- melt(data.frame(rownames(confusion.dat),round(confusion.dat,3)))
names(confusion.melt) <- c("Y","X","Count")

classErrorPlot(class.error, confusion.melt, 4, 11)
```

Another important metric from Random Forests is the Variable Importance, here presented using Gini, measuring the redution in Classification Error caused by the inclusion of each predictor, limited to the 30 most important predictors.

```{r, comment="Plot importance", echo=FALSE, message=FALSE, fig.align='center', fig.height=9}
predictorsNames = names(activities)
predictorsNames = str_replace_all(predictorsNames, "_", " ")
predictorsNames = sapply(predictorsNames, simpleCap)

#Variables Importance
d = data.frame(predictorsNames[1:52], round(importance(mForest,2)))
names(d) = c("Predictor","mdg")
rownames(d) = NULL

importancePlot(d[1:30,], "mdg", 11)
```


## Out of Sample Accuracy
```{r, results='hide'}
cMatrix = confusionMatrix(testingActivities$classe, predict(mForest, testingActivities[, -53]))

# Out of Sample Accuracy
Accuracy = round(cMatrix$overall[1], 4)

# 95% Confidence Interval
ConfInt = round(cMatrix$overall[c(3, 4)], 4)
```

The accuracy measured using the reserved testing set is `r Accuracy`, with a 95% Confidence Interval of (`r ConfInt`).

Below I present the Balanced Accuracy per Class, plus the measures of Sensitivity and Specificity.

```{r, echo=FALSE}
#g1 = tableGrob(round(cMatrix$byClass[, c("Sensitivity", "Specificity", "Balanced Accuracy")], 4))
#g2 = tableGrob(cMatrix$table, gpar.rowtext = gpar(col = "black", cex = 1, fontface = "bold"))

#grid.arrange(g1, g2, ncol=2)
kable(x = cMatrix$byClass[, c("Sensitivity", "Specificity", "Balanced Accuracy")], digits = 4, align = 'c')

#kable(cMatrix$table)
```

## Assignment Test Submission

Finally, here is the code to predict the answers for the Assignment Test Submission.

```{r, comment="Predict test data", results='hide'}
testActivities = read.csv("data/pml-testing.csv", header=T, comment.char="", quote="\"", col.names = activitiesNames)
# Drop all unwanted/unimportant columns
testActivities = testActivities[, -unwantedColumns]
testActivities = testActivities[, -which(names(testActivities) %in% otherUnwantedCols)]

answers = predict(mForest, testActivities[, -53])
```

## References
